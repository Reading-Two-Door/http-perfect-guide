## 공부 내용

### 크롤러와 크롤링

- 웹 크롤러는 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이즈를 가져오고 **재귀적으로 반복하는 방식으로 웹을 순회하는 로봇**이다.
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 부르며, 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사잍, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

#### 루프와 중복

- 순환은 크롤러를 계속 순환하게 할 수 있고, 같은 페이지를 반복해서 가져오는데 시간을 허비 할 수 있다.
- 크롤러의 네트워크 접근 속도가 충분히 빠르다면, 웹 사이트를 압박하여 어떤 실제 사용자도 사이트에 접근할 수 없도록 막아버릴 수 있다.

#### 웹 크롤러의 방문 관리 기법

- 트리와 해시 테이블
- 느슨한 존재 비트맵
- 체크포인트
- 파티셔닝

#### URL 정규화하기

- URL이 별칭을 가지는 경우 방문 기록이 쉽지 않다.

- 정규화
  1. 포트 번호가 명시되지 않았다면 80포트 추가
  2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
  3. "#" 태그들을 제거한다.
- 정규화 불가능한 케이스
  - 서버가 대소문자를 구분하지 않을 때
  - 기본 색인 페이지 설정을 알아야할 때
  - IP주소

#### 크롤링 봇이 올바르게 동작하기 위해 사용하는 기법

- URL 정규화 - URL을 표준 형태로 변환하여 중복 URL 일부 회피
- 너비 우선 크롤링 - 너비를 우선으로 스케쥴링하여 순환의 영향을 최소화
- 스로틀링 - 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
- URL 크기 제한 - 일정 길이를 넘는 URL의 크롤링은 거부
- URL/사이트 블랙리스트 - 문제를 일으키는 사이트를 블랙리스트에 추가하여 관리
- 패턴 발견 - 반복되는 패턴을 잠재적인 순환으로 간주하여 크롤링 거절
- 콘텐츠 지문 - 페이지의 컨텐츠에서 바이트를 얻어내서 체크섬을 계산
- 사람의 모니터링

### 로봇의 HTTP

- 로봇도 HTTP 명세를 지켜야한다. 많은 로봇이 컨텐츠 요청을 위한 HTTP를 최소한으로 구현하려해 HTTP/1.0을 자주 쓴다.

#### 권장 신원 식별 헤더

- User-Agent - 서버에게 요청을 만든 로봇의 이름
- From - 로봇의 사용자/관리자의 이메일 주소 제공
- Accept - 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
- Referer - 현재의 요청 URL 포함 문서 URL 제공

#### 가상 호스팅

- 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 컨텐츠를 찾게 만든다. 이러한 이유로 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.

## 알게된 점

## 궁금한 점
