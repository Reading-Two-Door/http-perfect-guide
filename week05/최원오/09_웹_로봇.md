## 공부 내용

### 크롤러와 크롤링

- 웹 크롤러는 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이즈를 가져오고 **재귀적으로 반복하는 방식으로 웹을 순회하는 로봇**이다.
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 부르며, 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사잍, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

#### 루프와 중복

- 순환은 크롤러를 계속 순환하게 할 수 있고, 같은 페이지를 반복해서 가져오는데 시간을 허비 할 수 있다.
- 크롤러의 네트워크 접근 속도가 충분히 빠르다면, 웹 사이트를 압박하여 어떤 실제 사용자도 사이트에 접근할 수 없도록 막아버릴 수 있다.

#### 웹 크롤러의 방문 관리 기법

- 트리와 해시 테이블
- 느슨한 존재 비트맵
- 체크포인트
- 파티셔닝

#### URL 정규화하기

- URL이 별칭을 가지는 경우 방문 기록이 쉽지 않다.

- 정규화
  1. 포트 번호가 명시되지 않았다면 80포트 추가
  2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
  3. "#" 태그들을 제거한다.
- 정규화 불가능한 케이스
  - 서버가 대소문자를 구분하지 않을 때
  - 기본 색인 페이지 설정을 알아야할 때
  - IP주소

#### 크롤링 봇이 올바르게 동작하기 위해 사용하는 기법

- URL 정규화 - URL을 표준 형태로 변환하여 중복 URL 일부 회피
- 너비 우선 크롤링 - 너비를 우선으로 스케쥴링하여 순환의 영향을 최소화
- 스로틀링 - 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
- URL 크기 제한 - 일정 길이를 넘는 URL의 크롤링은 거부
- URL/사이트 블랙리스트 - 문제를 일으키는 사이트를 블랙리스트에 추가하여 관리
- 패턴 발견 - 반복되는 패턴을 잠재적인 순환으로 간주하여 크롤링 거절
- 콘텐츠 지문 - 페이지의 컨텐츠에서 바이트를 얻어내서 체크섬을 계산
- 사람의 모니터링

### 로봇의 HTTP

- 로봇도 HTTP 명세를 지켜야한다. 많은 로봇이 컨텐츠 요청을 위한 HTTP를 최소한으로 구현하려해 HTTP/1.0을 자주 쓴다.

#### 권장 신원 식별 헤더

- User-Agent - 서버에게 요청을 만든 로봇의 이름
- From - 로봇의 사용자/관리자의 이메일 주소 제공
- Accept - 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다.
- Referer - 현재의 요청 URL 포함 문서 URL 제공

#### 가상 호스팅

- 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 컨텐츠를 찾게 만든다. 이러한 이유로 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.
- 가상호스팅으로 인해 관점이 다른 사이트로부터의 문서를 크롤링하게 될 수 있다.

#### 응답 다루기

- 상태 코드 - 로봇은 200, 404 같은 HTTP 상태코드를 이해해야 한다. 하지만 모든 서버가 적절한 에러 코드를 반환하는 것은 아니니 참고
- 엔터티 - http-equiv 태그는 콘텐츠 저자가 콘텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단
- User-Agent 타기팅 - 로봇의 요청을 다루기 위한 전략이 필요?

#### 부적절하게 동작하는 로봇

- 사람보다 훨씬 빠르게 HTTP 요청을 할 수 있기 때문에 순환에 빠지면 극심한 부하 발생
- 컨텐츠가 변경되어 존재하지 않는 URL에 대한 요청을 하는 경우
- 긴 URL일 경우 처리 능력에 영향을 주고 로그를 어지럽게 한다.
- 사적인 컨텐츠를 링크따라 이동하는 경우

### 로봇 차단하기

- robots.txt 파일로 서버의 어떤 부분에 접근할 수 있는지에 대한 정보 제공
- 로봇 차단 표준이 있으나 2.0의 경우 복잡하고 채택되지 못해 0.0과 완전히 0.0을 호환하는 1.0 표준을 주로 사용
- 가상 호스팅을 사용하는 경우 가상 docroot에 서로 다른 robots.txt 파일이 있을 수 있다.

#### 로봇 응답 코드

- 200 - 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려 할 때 그 규칙에 따른다
- 404 - 차단 규칙이 존재하지 않는다고 가정하고 제약 없이 사이트에 접근
- 401,403 - 사이트로의 접근이 완전히 제한
- 503 - 사이트의 리소스 검색을 잠시 미뤄야한다.
- 3XX - 리소스가 발견될 때 까지 리다이렉트를 따라간다.

#### robots.txt 파일 포맷

- 빈 줄, 주석 줄, 규칙 줄로 구성된다. 규칙 줄은 HTTP헤더처럼 생겼고 패턴 매칭을 위해 사용
- 각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝 문자로 끝난다.
- User-Agent - 각 레코드의 시작 줄로 로봇이 자신에 대응하는 줄을 못찾거나 \* 줄도 찾지 못했다면 대응하는 레코드가 없는 것이므로 접근 권한이 없다. (대소문자 구분 x)
- Disallow/Allow - URL경로를 명시적으로 금지, 허용하며 첫번쨰로 맞은 것이 사용되며 어떤 것도 일치하지 않은 경우도 허용된다.
- 명세가 발전함에 따라 다른 필드를 포함할 수 있다.
- 한 줄을 여러 줄로 나눠 적는 것은 허용되지 않는다.
- 주석은 파일의 어디서든 허용
- 0.0버전은 Allow 줄을 지원하지 않음

### 검색엔진

- 초기 검색엔진은 웹상의 문서 위치를 돕는 단순한 DB 였으나 오늘날 수십억 개의 페이지에서 사용자들이 생성하는 질의로 인한 부하를 다루기 위해 복잡한 크롤러가 필요하다.
- 풀 텍스트 색인 - 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스

## 알게된 점

### 풀 텍스트 색인(full-text indexing)

- 데이터베이스, 검색 엔진 등에서 사용되는 기술로, 텍스트 문서의 모든 단어를 색인화하여 빠른 텍스트 검색을 가능하게 합니다.

1. 토큰화(Tokenization): 전체 텍스트를 개별 단어나 구(phrase)로 분할합니다. 이 과정에서 불필요한 문자(예: 구두점)가 제거됩니다.
2. 정규화(Normalization): 단어들을 일관된 형태로 변환합니다. 예를 들어, 대소문자를 일치시키거나, 복수형 단어를 단수형으로 변환하는 등의 작업이 이루어집니다.
3. 색인 생성(Indexing): 각 단어에 대해 해당 단어가 나타난 위치와 빈도를 기록합니다. 이 정보는 후속 검색 시 해당 단어가 포함된 문서를 신속하게 찾아내는데 사용됩니다.
4. 검색(Searching): 사용자가 검색 질의(query)을 입력하면, 색인은 해당 질의에 일치하는 문서를 찾아 결과로 반환합니다.

### 관련도 랭킹

- 정보 검색 시스템에서 사용되는 알고리즘으로, 사용자의 검색 질의에 대한 결과를 가장 관련성이 높은 순서로 정렬하는 방법

1. 용어 빈도(Term Frequency, TF): 문서 내에서 검색 질의에 해당하는 단어나 구가 얼마나 자주 나타나는지를 측정합니다. 단어가 자주 등장할수록 그 문서가 해당 질의와 관련성이 높다고 판단합니다.
2. 문서 빈도(Inverse Document Frequency, IDF): 전체 문서 집합에서 특정 단어가 나타나는 문서의 비율을 역수로 취한 값입니다. 모든 문서에서 널리 사용되는 단어(예: 'the', 'is' 등)는 특정 주제와 크게 관련성이 없다고 판단하여 가중치를 낮춥니다.
3. 문맥(Context): 검색 질의와 일치하는 단어나 구가 문서 내에서 어떤 위치에 있는지, 어떤 다른 단어들과 함께 위치하고 있는지 등의 정보를 고려합니다.
4. 사용자 행동(User Behavior): 이전 사용자들이 같은 혹은 유사한 검색 질의로 어떤 결과를 선택했는지, 그 결과에 만족했는지 등의 정보를 활용하기도 합니다.
5. 문서 구조(Document Structure): 제목, 하위 제목, 본문 등 각 섹션에 위치한 내용은 서로 다른 중요도를 가집니다.

### 스푸핑

- 컴퓨터 보안에서 일반적으로 사용되는 용어로, 특정 개체나 사용자를 다른 사람이나 시스템으로 위장하는 행위

1. IP 스푸핑: 공격자가 IP 패킷의 소스 주소를 조작하여 네트워크 상에서 다른 컴퓨터로 보이게 만드는 방식입니다.
2. ARP 스푸핑: 공격자가 ARP(Address Resolution Protocol) 메시지를 조작하여 네트워크 내의 다른 기기들로부터 트래픽을 가로채는 방식입니다.
3. DNS 스푸핑: DNS(Domain Name System) 쿼리에 대한 응답을 조작하여 사용자를 악성 웹사이트로 유도하는 방식입니다.
4. 이메일 스푸핑: 이메일 발신인 정보를 조작하여 수신인에게 자신을 신뢰할 수 있는 발신인으로 위장하는 방식입니다.
5. 콜러 ID 스푸핑: 전화번호 정보를 조작하여 수신인에게 자신을 신뢰할 수 있는 발신인으로 위장하는 방식입니다.

## 궁금한 점
